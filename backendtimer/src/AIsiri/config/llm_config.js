'use strict';

const OpenAI = require('openai');
const path = require('path');
const fs = require('fs');

class LLMConfig {
  constructor() {
    this.mockMode = process.env.MOCK_LLM === 'true' || process.env.NODE_ENV === 'test';
    this.loadAPIKey();
    this.initializeClient();
  }

  loadAPIKey() {
    try {
      if (this.mockMode) {
        this.apiKey = 'mock-api-key';
        this.model = process.env.LLM_MODEL || 'qwen-plus';
        console.log('ğŸ§ª MOCK LLM æ¨¡å¼å·²å¯ç”¨');
        return;
      }

      // è¯»å–APIå¯†é’¥æ–‡ä»¶ï¼ˆä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
      const envKey = process.env.OPENAI_API_KEY || process.env.DASHSCOPE_API_KEY;
      const envModel = process.env.LLM_MODEL;
      if (envKey) {
        this.apiKey = envKey.trim();
        this.model = (envModel || 'qwen-plus').trim();
      } else {
        const apiKeyPath = path.join(__dirname, '../../../doc/APIkey');
        const apiKeyContent = fs.readFileSync(apiKeyPath, 'utf8');
        const lines = apiKeyContent.trim().split('\n');
        this.apiKey = lines[0].trim();
        this.model = (lines[2] ? lines[2].trim() : 'qwen-plus');
      }

      console.log('âœ… APIé…ç½®åŠ è½½æˆåŠŸ');
      console.log(`ğŸ“ ä½¿ç”¨æ¨¡å‹: ${this.model}`);
    } catch (error) {
      console.error('âŒ æ— æ³•åŠ è½½APIå¯†é’¥:', error.message);
      throw error;
    }
  }

  initializeClient() {
    try {
      if (this.mockMode) {
        this.client = {
          chat: {
            completions: {
              create: async (options) => {
                // ç®€å•çš„mockå“åº”
                return {
                  choices: [{ message: { content: '{"mock":true}' } }],
                  usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 }
                };
              }
            }
          }
        };
        console.log('ğŸ§ª ä½¿ç”¨ Mock OpenAI å®¢æˆ·ç«¯');
        return;
      }

      this.client = new OpenAI({
        apiKey: this.apiKey,
        baseURL: process.env.OPENAI_BASE_URL || 'https://dashscope.aliyuncs.com/compatible-mode/v1'
      });
      console.log('âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ');
    } catch (error) {
      console.error('âŒ OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥:', error.message);
      throw error;
    }
  }

  async testConnection() {
    try {
      console.log('ğŸ”— æµ‹è¯•LLMè¿æ¥...');
      const response = await this.client.chat.completions.create({
        model: this.model,
        messages: [
          { role: "system", content: "You are a helpful assistant." },
          { role: "user", content: "Hello, please respond with 'Connection successful!'" }
        ],
        max_tokens: 50
      });
      
      console.log('âœ… LLMè¿æ¥æµ‹è¯•æˆåŠŸ');
      console.log('ğŸ“¤ å“åº”:', response.choices[0].message.content);
      return true;
    } catch (error) {
      console.error('âŒ LLMè¿æ¥æµ‹è¯•å¤±è´¥:', error.message);
      return false;
    }
  }

  getClient() {
    return this.client;
  }

  getModel() {
    return this.model;
  }
}

module.exports = LLMConfig;